{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30664,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T19:11:48.724587Z",
     "start_time": "2024-03-23T19:11:41.770023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "if tf.test.gpu_device_name(): \n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install GPU version of TF\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "import fiftyone as fo\nimport pandas as pd\nfrom fiftyone import ViewField as F\n\nmy_classes = [\"Frog\", \"Fish\", \"Bird\"]\nexport_dir = \"/kaggle/cv/p6\"\n\ndataset = fo.zoo.load_zoo_dataset(\n    \"open-images-v7\",\n    split=\"train\",\n    label_types=[\"detections\"],\n    classes=my_classes,\n    max_samples=1000,\n    shuffle=True\n)\n\ndataset = dataset.filter_labels(\"ground_truth\", F(\"label\").is_in(my_classes))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:36:44.900525Z",
     "iopub.execute_input": "2024-03-23T18:36:44.900917Z",
     "iopub.status.idle": "2024-03-23T18:38:38.187604Z",
     "shell.execute_reply.started": "2024-03-23T18:36:44.900879Z",
     "shell.execute_reply": "2024-03-23T18:38:38.186726Z"
    },
    "trusted": true
   },
   "execution_count": 204,
   "outputs": [
    {
     "name": "stdout",
     "text": "Downloading split 'train' to '/root/fiftyone/open-images-v7/train' if necessary\nNecessary images already downloaded\nExisting download of split 'train' is sufficient\nLoading existing dataset 'open-images-v7-train-1000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Calculate the number of samples to include in the training set\nnum_samples = len(dataset)\ntrain_samples = int(num_samples * 0.7)\n\n# Split the dataset\ntrain_dataset = dataset.take(train_samples)\ntest_dataset = dataset.skip(train_samples)\n\n#view the datasets to verify the split\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Testing dataset size: {len(test_dataset)}\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:38:38.188693Z",
     "iopub.execute_input": "2024-03-23T18:38:38.188959Z",
     "iopub.status.idle": "2024-03-23T18:38:38.229144Z",
     "shell.execute_reply.started": "2024-03-23T18:38:38.188934Z",
     "shell.execute_reply": "2024-03-23T18:38:38.228280Z"
    },
    "trusted": true
   },
   "execution_count": 205,
   "outputs": [
    {
     "name": "stdout",
     "text": "Training dataset size: 700\nTesting dataset size: 300\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "train_dir = \"/kaggle/cv/train\"\ntest_dir = \"/kaggle/cv/test\"\n\ntrain_patches = train_dataset.to_patches(\"ground_truth\")\n\ntrain_patches.export(\n    export_dir=train_dir,\n    dataset_type=fo.types.ImageClassificationDirectoryTree,\n    label_field=\"ground_truth\",\n)\n\ntest_patches = test_dataset.to_patches(\"ground_truth\")\n\ntest_patches.export(\n    export_dir=test_dir,\n    dataset_type=fo.types.ImageClassificationDirectoryTree,\n    label_field=\"ground_truth\",\n)\n\nif os.path.exists(\"/kaggle/cv/train/augmented\"):\n    shutil.rmtree(\"/kaggle/cv/train/augmented\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:38:38.231363Z",
     "iopub.execute_input": "2024-03-23T18:38:38.231645Z",
     "iopub.status.idle": "2024-03-23T18:39:24.514071Z",
     "shell.execute_reply.started": "2024-03-23T18:38:38.231614Z",
     "shell.execute_reply": "2024-03-23T18:39:24.512948Z"
    },
    "trusted": true
   },
   "execution_count": 206,
   "outputs": [
    {
     "name": "stdout",
     "text": "Directory '/kaggle/cv/train' already exists; export will be merged with existing files\nDetected an image classification exporter and a label field 'ground_truth' of type <class 'fiftyone.core.labels.Detection'>. Exporting image patches...\n 100% |███████████████| 1842/1842 [32.6s elapsed, 0s remaining, 56.8 samples/s]      \nDirectory '/kaggle/cv/test' already exists; export will be merged with existing files\nDetected an image classification exporter and a label field 'ground_truth' of type <class 'fiftyone.core.labels.Detection'>. Exporting image patches...\n 100% |█████████████████| 791/791 [13.2s elapsed, 0s remaining, 66.5 samples/s]      \n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!ls /kaggle/cv/train",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:39:24.515342Z",
     "iopub.execute_input": "2024-03-23T18:39:24.515638Z",
     "iopub.status.idle": "2024-03-23T18:39:25.615767Z",
     "shell.execute_reply.started": "2024-03-23T18:39:24.515608Z",
     "shell.execute_reply": "2024-03-23T18:39:25.614535Z"
    },
    "trusted": true
   },
   "execution_count": 207,
   "outputs": [
    {
     "name": "stdout",
     "text": "Bird  Fish  Frog\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!echo \"Bird files:\" && ls -1 /kaggle/cv/train/Bird | wc -l\n!echo \"Fish files:\" && ls -1 /kaggle/cv/train/Fish | wc -l\n!echo \"Frog files:\" && ls -1 /kaggle/cv/train/Frog | wc -l",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:39:25.617503Z",
     "iopub.execute_input": "2024-03-23T18:39:25.617826Z",
     "iopub.status.idle": "2024-03-23T18:39:28.935351Z",
     "shell.execute_reply.started": "2024-03-23T18:39:25.617795Z",
     "shell.execute_reply": "2024-03-23T18:39:28.934251Z"
    },
    "trusted": true
   },
   "execution_count": 208,
   "outputs": [
    {
     "name": "stdout",
     "text": "Bird files:\n1350\nFish files:\n609\nFrog files:\n51\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!ls /kaggle/cv/test",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:39:28.937047Z",
     "iopub.execute_input": "2024-03-23T18:39:28.937377Z",
     "iopub.status.idle": "2024-03-23T18:39:30.039136Z",
     "shell.execute_reply.started": "2024-03-23T18:39:28.937348Z",
     "shell.execute_reply": "2024-03-23T18:39:30.038043Z"
    },
    "trusted": true
   },
   "execution_count": 209,
   "outputs": [
    {
     "name": "stdout",
     "text": "Bird  Fish  Frog\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!echo \"Bird files:\" && ls -1 /kaggle/cv/test/Bird | wc -l\n!echo \"Fish files:\" && ls -1 /kaggle/cv/test/Fish | wc -l\n!echo \"Frog files:\" && ls -1 /kaggle/cv/test/Frog | wc -l",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:39:30.041315Z",
     "iopub.execute_input": "2024-03-23T18:39:30.041745Z",
     "iopub.status.idle": "2024-03-23T18:39:33.385318Z",
     "shell.execute_reply.started": "2024-03-23T18:39:30.041704Z",
     "shell.execute_reply": "2024-03-23T18:39:33.384032Z"
    },
    "trusted": true
   },
   "execution_count": 210,
   "outputs": [
    {
     "name": "stdout",
     "text": "Bird files:\n508\nFish files:\n263\nFrog files:\n20\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "print(\"clearly there is class imbalance, cleaning and rebalancing is done in step4:Experiment data cleansing\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:39:33.387202Z",
     "iopub.execute_input": "2024-03-23T18:39:33.387527Z",
     "iopub.status.idle": "2024-03-23T18:39:33.393230Z",
     "shell.execute_reply.started": "2024-03-23T18:39:33.387497Z",
     "shell.execute_reply": "2024-03-23T18:39:33.392298Z"
    },
    "trusted": true
   },
   "execution_count": 211,
   "outputs": [
    {
     "name": "stdout",
     "text": "clearly there is class imbalance, cleaning and rebalancing is done in step4:Experiment data cleansing\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "print(\"VGG19 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper -Very Deep Convolutional Networks for Large-Scale Image Recognition-. The model is 19 layers deep and was trained on the ImageNet dataset, which contains millions of images classified into 1000 categories.\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:39:33.397751Z",
     "iopub.execute_input": "2024-03-23T18:39:33.398050Z",
     "iopub.status.idle": "2024-03-23T18:39:33.406737Z",
     "shell.execute_reply.started": "2024-03-23T18:39:33.398021Z",
     "shell.execute_reply": "2024-03-23T18:39:33.405646Z"
    },
    "trusted": true
   },
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "text": "VGG19 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper -Very Deep Convolutional Networks for Large-Scale Image Recognition-. The model is 19 layers deep and was trained on the ImageNet dataset, which contains millions of images classified into 1000 categories.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\ndef build_vgg19_model(input_shape=(224, 224, 3), num_classes=3):\n    model = Sequential([\n        Input(shape=input_shape),  # Define the input shape explicitly with an Input layer\n        # Block 1\n        Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1'),\n        Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2'),\n        MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool'),\n        \n        # Block 2\n        Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1'),\n        Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2'),\n        MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool'),\n        \n        # Block 3\n        Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1'),\n        Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2'),\n        Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3'),\n        Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv4'),\n        MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool'),\n        \n        # Block 4\n        Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1'),\n        Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2'),\n        Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3'),\n        Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv4'),\n        MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool'),\n        \n        # Block 5\n        Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1'),\n        Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2'),\n        Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3'),\n        Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv4'),\n        MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool'),\n        \n        # Classification block\n        Flatten(),\n        Dense(4096, activation='relu', name='fc1'),\n        Dropout(0.5),\n        Dense(4096, activation='relu', name='fc2'),\n        Dropout(0.5),\n        Dense(num_classes, activation='softmax', name='predictions')\n    ])\n    \n    return model\n\n# compile the model with this new definition\nmodel = build_vgg19_model(input_shape=(224, 224, 3), num_classes=3)\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:39:33.408096Z",
     "iopub.execute_input": "2024-03-23T18:39:33.408339Z",
     "iopub.status.idle": "2024-03-23T18:39:33.594366Z",
     "shell.execute_reply.started": "2024-03-23T18:39:33.408318Z",
     "shell.execute_reply": "2024-03-23T18:39:33.593435Z"
    },
    "trusted": true
   },
   "execution_count": 213,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_dir = \"/kaggle/cv/train\"\ntest_dir = \"/kaggle/cv/test\"\n\ntrain_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(224, 224),  # Size of input images expected by VGG19\n    batch_size=32,\n    class_mode='categorical')  # Use 'binary' if you have two classes\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical',  # Use 'binary' if you have two classes\n    shuffle=False)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:39:33.595587Z",
     "iopub.execute_input": "2024-03-23T18:39:33.595867Z",
     "iopub.status.idle": "2024-03-23T18:39:33.684796Z",
     "shell.execute_reply.started": "2024-03-23T18:39:33.595843Z",
     "shell.execute_reply": "2024-03-23T18:39:33.683875Z"
    },
    "trusted": true
   },
   "execution_count": 214,
   "outputs": [
    {
     "name": "stdout",
     "text": "Found 2010 images belonging to 3 classes.\nFound 791 images belonging to 3 classes.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Train the model\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n    epochs=10,  # As per requirements\n    validation_data=test_generator,\n    validation_steps=test_generator.samples // test_generator.batch_size\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:39:33.685872Z",
     "iopub.execute_input": "2024-03-23T18:39:33.686169Z",
     "iopub.status.idle": "2024-03-23T18:43:11.671818Z",
     "shell.execute_reply.started": "2024-03-23T18:39:33.686145Z",
     "shell.execute_reply": "2024-03-23T18:43:11.670752Z"
    },
    "trusted": true
   },
   "execution_count": 215,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/10\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nW0000 00:00:1711219184.130470    2245 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m11/62\u001B[0m \u001B[32m━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m23s\u001B[0m 467ms/step - accuracy: 0.4405 - loss: 1.1400",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "W0000 00:00:1711219232.954850    2242 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 1s/step - accuracy: 0.5830 - loss: 0.9042",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "W0000 00:00:1711219256.927369    2243 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m86s\u001B[0m 1s/step - accuracy: 0.5840 - loss: 0.9024 - val_accuracy: 0.6615 - val_loss: 0.6741\nEpoch 2/10\n\u001B[1m 1/62\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m27s\u001B[0m 447ms/step - accuracy: 0.7812 - loss: 0.6438",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 10ms/step - accuracy: 0.7812 - loss: 0.6438 - val_accuracy: 0.0000e+00 - val_loss: 3.8640\nEpoch 3/10\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "W0000 00:00:1711219261.160525    2245 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 513ms/step - accuracy: 0.6642 - loss: 0.7560 - val_accuracy: 0.6615 - val_loss: 0.6765\nEpoch 4/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.7692 - loss: 0.6089 - val_accuracy: 0.0000e+00 - val_loss: 3.1445\nEpoch 5/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m32s\u001B[0m 507ms/step - accuracy: 0.6622 - loss: 0.7328 - val_accuracy: 0.6615 - val_loss: 0.6787\nEpoch 6/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.6562 - loss: 0.6955 - val_accuracy: 0.0000e+00 - val_loss: 3.5154\nEpoch 7/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 498ms/step - accuracy: 0.6733 - loss: 0.7250 - val_accuracy: 0.6615 - val_loss: 0.6562\nEpoch 8/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 2ms/step - accuracy: 0.7500 - loss: 0.7918 - val_accuracy: 0.0000e+00 - val_loss: 3.7627\nEpoch 9/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 498ms/step - accuracy: 0.6671 - loss: 0.7329 - val_accuracy: 0.6615 - val_loss: 0.6662\nEpoch 10/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 21ms/step - accuracy: 0.5625 - loss: 0.9454 - val_accuracy: 0.0000e+00 - val_loss: 3.5611\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "# Using a pretrained model\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Load the base VGG19 model, without the top layer\nbase_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the layers of the base model\nfor layer in base_model.layers:\n    layer.trainable = False\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:43:11.673691Z",
     "iopub.execute_input": "2024-03-23T18:43:11.674579Z",
     "iopub.status.idle": "2024-03-23T18:43:12.006680Z",
     "shell.execute_reply.started": "2024-03-23T18:43:11.674538Z",
     "shell.execute_reply": "2024-03-23T18:43:12.005808Z"
    },
    "trusted": true
   },
   "execution_count": 216,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Extending the base model,number of units in the Dense layer = number of classes =3\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.5)(x)  # Add dropout to reduce overfitting\npredictions = Dense(3, activation='softmax')(x)\n\n# This is the model we will train\nmodel = Model(inputs=base_model.input, outputs=predictions)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:43:12.007998Z",
     "iopub.execute_input": "2024-03-23T18:43:12.008343Z",
     "iopub.status.idle": "2024-03-23T18:43:12.037879Z",
     "shell.execute_reply.started": "2024-03-23T18:43:12.008312Z",
     "shell.execute_reply": "2024-03-23T18:43:12.037015Z"
    },
    "trusted": true
   },
   "execution_count": 217,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from tensorflow.keras.optimizers import Adam\n\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:43:12.039081Z",
     "iopub.execute_input": "2024-03-23T18:43:12.039385Z",
     "iopub.status.idle": "2024-03-23T18:43:12.047664Z",
     "shell.execute_reply.started": "2024-03-23T18:43:12.039362Z",
     "shell.execute_reply": "2024-03-23T18:43:12.046802Z"
    },
    "trusted": true
   },
   "execution_count": 218,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        train_dir,\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        test_dir,\n        target_size=(224, 224),\n        batch_size=32,\n        class_mode='categorical')\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:43:12.049161Z",
     "iopub.execute_input": "2024-03-23T18:43:12.049466Z",
     "iopub.status.idle": "2024-03-23T18:43:12.133382Z",
     "shell.execute_reply.started": "2024-03-23T18:43:12.049443Z",
     "shell.execute_reply": "2024-03-23T18:43:12.132545Z"
    },
    "trusted": true
   },
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "text": "Found 2010 images belonging to 3 classes.\nFound 791 images belonging to 3 classes.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "history = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n    epochs=10,\n    validation_data=validation_generator,\n    validation_steps=validation_generator.samples // validation_generator.batch_size)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:43:12.134529Z",
     "iopub.execute_input": "2024-03-23T18:43:12.134785Z",
     "iopub.status.idle": "2024-03-23T18:45:26.193074Z",
     "shell.execute_reply.started": "2024-03-23T18:43:12.134763Z",
     "shell.execute_reply": "2024-03-23T18:45:26.192241Z"
    },
    "trusted": true
   },
   "execution_count": 220,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/10\n\u001B[1m 1/62\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m6:11\u001B[0m 6s/step - accuracy: 0.4062 - loss: 1.1398",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "W0000 00:00:1711219398.386616    2245 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m28/62\u001B[0m \u001B[32m━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━\u001B[0m \u001B[1m13s\u001B[0m 399ms/step - accuracy: 0.5438 - loss: 0.9183",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "W0000 00:00:1711219409.187353    2244 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 334ms/step - accuracy: 0.5843 - loss: 0.8555",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "W0000 00:00:1711219419.888248    2242 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m31s\u001B[0m 407ms/step - accuracy: 0.5851 - loss: 0.8543 - val_accuracy: 0.6406 - val_loss: 0.6770\nEpoch 2/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 10ms/step - accuracy: 0.7812 - loss: 0.6250 - val_accuracy: 0.6957 - val_loss: 0.5931\nEpoch 3/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 366ms/step - accuracy: 0.6718 - loss: 0.7312 - val_accuracy: 0.6471 - val_loss: 0.6342\nEpoch 4/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.6562 - loss: 0.6666 - val_accuracy: 0.6522 - val_loss: 0.5664\nEpoch 5/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 371ms/step - accuracy: 0.6967 - loss: 0.6708 - val_accuracy: 0.6797 - val_loss: 0.5891\nEpoch 6/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7500 - loss: 0.6096 - val_accuracy: 0.6522 - val_loss: 0.6136\nEpoch 7/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 369ms/step - accuracy: 0.7361 - loss: 0.6077 - val_accuracy: 0.7617 - val_loss: 0.5570\nEpoch 8/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7500 - loss: 0.5706 - val_accuracy: 0.7391 - val_loss: 0.5251\nEpoch 9/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 365ms/step - accuracy: 0.7270 - loss: 0.6040 - val_accuracy: 0.8516 - val_loss: 0.5435\nEpoch 10/10\n\u001B[1m62/62\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 2ms/step - accuracy: 0.7500 - loss: 0.6340 - val_accuracy: 0.9565 - val_loss: 0.3984\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Experiment with Data Cleansing",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Defined automatic approach to deleting blurry, under/overexposed images and noisy images",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "pip install opencv-python-headless",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:45:26.194427Z",
     "iopub.execute_input": "2024-03-23T18:45:26.194720Z",
     "iopub.status.idle": "2024-03-23T18:45:38.727796Z",
     "shell.execute_reply.started": "2024-03-23T18:45:26.194694Z",
     "shell.execute_reply": "2024-03-23T18:45:38.726560Z"
    },
    "trusted": true
   },
   "execution_count": 221,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: opencv-python-headless in /opt/conda/lib/python3.10/site-packages (4.9.0.80)\nRequirement already satisfied: numpy>=1.21.2 in /opt/conda/lib/python3.10/site-packages (from opencv-python-headless) (1.26.4)\nNote: you may need to restart the kernel to use updated packages.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import cv2 as cv\nimport numpy as np\n\ndef is_blurry(image_obj, threshold=120):\n    image = np.array(image_obj)\n    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n    variance = cv.Laplacian(gray, cv.CV_64F).var()\n    return variance < threshold\n\ndef is_poorly_exposed(image_obj, low_threshold=10, high_threshold=245, ratio_threshold=0.17):\n    image = np.array(image_obj)\n    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n    hist = cv.calcHist([gray], [0], None, [256], [0, 256])\n    \n    low_light = np.sum(hist[:low_threshold])\n    high_light = np.sum(hist[high_threshold:])\n    \n    total_pixels = gray.size\n    return (low_light/total_pixels > ratio_threshold) or (high_light/total_pixels > ratio_threshold)\n\n\ndef is_noisy(image_obj, s_thr=0.15):\n    image = np.array(image_obj)\n    # Convert image to HSV color space\n    image = cv.cvtColor(image, cv.COLOR_BGR2HSV)\n\n    # Calculate histogram of saturation channel\n    s = cv.calcHist([image], [1], None, [256], [0, 256])\n\n    # Calculate percentage of pixels with high saturation\n    p = 0.05  # Considering pixels with saturation >= p\n    s_perc = np.sum(s[int(p * 255):]) / np.prod(image.shape[0:2])\n\n    return s_perc < s_thr  # Adjusted logic and threshold",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:45:38.729602Z",
     "iopub.execute_input": "2024-03-23T18:45:38.729944Z",
     "iopub.status.idle": "2024-03-23T18:45:38.742206Z",
     "shell.execute_reply.started": "2024-03-23T18:45:38.729911Z",
     "shell.execute_reply": "2024-03-23T18:45:38.741247Z"
    },
    "trusted": true
   },
   "execution_count": 222,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "In addition duplicate images and images that are below the 32x32 minimum resolution of vgg19 will be removed",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nfrom PIL import Image\nimport imagehash\n\ndef remove_bad_images(dataset_dir, min_width=10, min_height=10):\n    hashes = {}\n    duplicates = []\n    corrupted_or_small = []\n    removed_files = []\n    blurry = []\n    noisy = []\n    poor_exposure = []\n\n    for fileclass in os.listdir(dataset_dir):\n        print(fileclass)\n        category_dir = dataset_dir + \"/\" + fileclass\n        \n        for filename in os.listdir(category_dir):\n            \n            filepath = os.path.join(category_dir, filename)\n            try:\n                with Image.open(filepath) as img:\n                    # Check for low resolution\n                    if img.width < min_width or img.height < min_height:\n                        corrupted_or_small.append(filename)\n                        continue\n                    \n                    if is_noisy(img):\n                        noisy.append(filename)\n                        continue\n                        \n                    if is_blurry(img):\n                        blurry.append(filename)\n                        continue\n                    \n                    if is_poorly_exposed(img):\n                        poor_exposure.append(filename)\n                        continue\n                    \n    \n                    # Compute hash for duplicate detection\n                    hash = imagehash.average_hash(img)\n                    if hash in hashes:\n                        duplicates.append(filename)\n                        continue\n                    else:\n                        hashes[hash] = filename\n                        \n            except cv.error as e:\n                corrupted_or_small.append(filename)\n                continue\n        \n            except (IOError, OSError):\n                corrupted_or_small.append(filename)\n                continue\n                \n        # Remove corrupted or small images\n        for filename in corrupted_or_small + duplicates + poor_exposure + blurry + noisy:\n            try:\n                os.remove(os.path.join(category_dir, filename))\n                removed_files.append(filename)\n            except (IOError):\n                continue\n        print(f\"Found  {len(noisy)} noisy images\")\n        print(f\"Found  {len(blurry)} blurry images\")\n        print(f\"Found  {len(poor_exposure)} bad exposure images\")\n        print(f\"Found  {len(duplicates)} duplicate images\")\n        print(f\"Found  {len(corrupted_or_small)} too small res images\")\n\n    return removed_files\n\n\ntrain_removed_files = remove_bad_images(train_dir)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:45:38.743411Z",
     "iopub.execute_input": "2024-03-23T18:45:38.743685Z",
     "iopub.status.idle": "2024-03-23T18:45:45.405854Z",
     "shell.execute_reply.started": "2024-03-23T18:45:38.743661Z",
     "shell.execute_reply": "2024-03-23T18:45:45.404825Z"
    },
    "trusted": true
   },
   "execution_count": 223,
   "outputs": [
    {
     "name": "stdout",
     "text": "Fish\nFound  0 noisy images\nFound  143 blurry images\nFound  12 bad exposure images\nFound  76 duplicate images\nFound  19 too small res images\nBird\nFound  13 noisy images\nFound  256 blurry images\nFound  98 bad exposure images\nFound  82 duplicate images\nFound  85 too small res images\nFrog\nFound  13 noisy images\nFound  264 blurry images\nFound  105 bad exposure images\nFound  82 duplicate images\nFound  85 too small res images\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!echo \"Bird files:\" && ls -1 /kaggle/cv/train/Bird | wc -l\n!echo \"Fish files:\" && ls -1 /kaggle/cv/train/Fish | wc -l\n!echo \"Frog files:\" && ls -1 /kaggle/cv/train/Frog | wc -l",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:45:45.407386Z",
     "iopub.execute_input": "2024-03-23T18:45:45.407677Z",
     "iopub.status.idle": "2024-03-23T18:45:48.784658Z",
     "shell.execute_reply.started": "2024-03-23T18:45:45.407653Z",
     "shell.execute_reply": "2024-03-23T18:45:48.783398Z"
    },
    "trusted": true
   },
   "execution_count": 224,
   "outputs": [
    {
     "name": "stdout",
     "text": "Bird files:\n864\nFish files:\n359\nFrog files:\n19\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!echo \"Bird files:\" && ls -1 /kaggle/cv/test/Bird | wc -l\n!echo \"Fish files:\" && ls -1 /kaggle/cv/test/Fish | wc -l\n!echo \"Frog files:\" && ls -1 /kaggle/cv/test/Frog | wc -l",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:45:48.786696Z",
     "iopub.execute_input": "2024-03-23T18:45:48.787112Z",
     "iopub.status.idle": "2024-03-23T18:45:52.108592Z",
     "shell.execute_reply.started": "2024-03-23T18:45:48.787071Z",
     "shell.execute_reply": "2024-03-23T18:45:52.107349Z"
    },
    "trusted": true
   },
   "execution_count": 225,
   "outputs": [
    {
     "name": "stdout",
     "text": "Bird files:\n508\nFish files:\n263\nFrog files:\n20\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Class balancing via kaggle",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from keras.preprocessing.image import load_img, img_to_array, save_img\n\ndef upsample_minority_class(minority_class_dir, augmented_class_dir, target_number):\n    # Make the directory if it doesn't exist\n    if os.path.exists(augmented_class_dir):\n        shutil.rmtree(augmented_class_dir)\n    os.makedirs(augmented_class_dir)\n    print(f\"The directory {augmented_class_dir} has been created\")\n\n    # List all files in the minority class directory\n    minority_images = [os.path.join(minority_class_dir, f) for f in os.listdir(minority_class_dir)]\n\n    num_new_images = target_number - len(minority_images)\n\n    # Generate new images using data augmentation\n    for i in range(num_new_images):\n        img_path = np.random.choice(minority_images)\n        img = load_img(img_path)\n        img_array = img_to_array(img)  \n        img_array = img_array.reshape((1,) + img_array.shape)\n        \n        for batch in augmentation_datagen.flow(img_array, batch_size=1, save_to_dir=augmented_class_dir, save_prefix='aug_', save_format='jpeg'):\n            break\n    \n    if os.path.exists(minority_class_dir):\n        shutil.rmtree(minority_class_dir)\n        print(f\"The directory {minority_class_dir} has been removed\")\n    else:\n        print(f\"The directory {minority_class_dir} does not exist\")\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:45:52.110631Z",
     "iopub.execute_input": "2024-03-23T18:45:52.110970Z",
     "iopub.status.idle": "2024-03-23T18:45:52.121427Z",
     "shell.execute_reply.started": "2024-03-23T18:45:52.110939Z",
     "shell.execute_reply": "2024-03-23T18:45:52.120490Z"
    },
    "trusted": true
   },
   "execution_count": 226,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "minority_class_dir = '/kaggle/cv/train/Frog/'\naugmented_class_dir = '/kaggle/cv/train/augmented/Frog/'\ntarget_number = 350\n\nupsample_minority_class(minority_class_dir, augmented_class_dir, target_number)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:45:52.122590Z",
     "iopub.execute_input": "2024-03-23T18:45:52.122834Z",
     "iopub.status.idle": "2024-03-23T18:46:03.969206Z",
     "shell.execute_reply.started": "2024-03-23T18:45:52.122813Z",
     "shell.execute_reply": "2024-03-23T18:46:03.968239Z"
    },
    "trusted": true
   },
   "execution_count": 227,
   "outputs": [
    {
     "name": "stdout",
     "text": "The directory /kaggle/cv/train/augmented/Frog/ has been created\nThe directory /kaggle/cv/train/Frog/ has been removed\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "!echo \"Bird files:\" && ls -1 /kaggle/cv/train/Bird | wc -l\n!echo \"Fish files:\" && ls -1 /kaggle/cv/train/Fish | wc -l\n!echo \"Frog files:\" && ls -1 /kaggle/cv/train/augmented/Frog | wc -l",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:46:03.970708Z",
     "iopub.execute_input": "2024-03-23T18:46:03.971618Z",
     "iopub.status.idle": "2024-03-23T18:46:07.310201Z",
     "shell.execute_reply.started": "2024-03-23T18:46:03.971582Z",
     "shell.execute_reply": "2024-03-23T18:46:07.309047Z"
    },
    "trusted": true
   },
   "execution_count": 228,
   "outputs": [
    {
     "name": "stdout",
     "text": "Bird files:\n864\nFish files:\n359\nFrog files:\n329\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Perform Data Augmentation and Class balancing",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n# Set up data augmentation for training data\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    horizontal_flip=True,\n    vertical_flip=True,\n    rotation_range=30,\n    zoom_range=0.2,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.1,\n    brightness_range=[0.4,1.5],\n    fill_mode='nearest'\n)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    \n    target_size=(224, 224),  \n    batch_size=32,\n    class_mode='categorical'\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(224, 224), \n    batch_size=32,\n    class_mode='categorical',\n    shuffle=False  \n)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:46:07.312378Z",
     "iopub.execute_input": "2024-03-23T18:46:07.312825Z",
     "iopub.status.idle": "2024-03-23T18:46:07.399494Z",
     "shell.execute_reply.started": "2024-03-23T18:46:07.312782Z",
     "shell.execute_reply": "2024-03-23T18:46:07.398750Z"
    },
    "trusted": true
   },
   "execution_count": 229,
   "outputs": [
    {
     "name": "stdout",
     "text": "Found 1552 images belonging to 3 classes.\nFound 791 images belonging to 3 classes.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": "Automatic approach to deleting blurry, under/overexposed images and noisy images",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from sklearn.utils.class_weight import compute_class_weight\nimport numpy as np\n\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_generator.classes),\n    y=train_generator.classes)\n\nclass_weights_dict = {i: weight for i, weight in enumerate(class_weights)}",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:46:07.404655Z",
     "iopub.execute_input": "2024-03-23T18:46:07.405024Z",
     "iopub.status.idle": "2024-03-23T18:46:07.412178Z",
     "shell.execute_reply.started": "2024-03-23T18:46:07.405001Z",
     "shell.execute_reply": "2024-03-23T18:46:07.411179Z"
    },
    "trusted": true
   },
   "execution_count": 230,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from tensorflow.keras.applications import VGG19\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.metrics import Precision, Recall\n\n\nbase_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n\nfor layer in base_model.layers:\n    layer.trainable = False\n\n\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.5)(x)  # Add dropout to reduce overfitting\npredictions = Dense(3, activation='softmax')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\nmodel.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall()])\n\n\nmodel.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n    epochs=21,\n    validation_data=test_generator,\n    validation_steps=test_generator.samples // test_generator.batch_size,\n    class_weight=class_weights_dict  # Apply class weights\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:46:07.413331Z",
     "iopub.execute_input": "2024-03-23T18:46:07.413672Z",
     "iopub.status.idle": "2024-03-23T18:50:50.548145Z",
     "shell.execute_reply.started": "2024-03-23T18:46:07.413636Z",
     "shell.execute_reply": "2024-03-23T18:50:50.546969Z"
    },
    "trusted": true
   },
   "execution_count": 231,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/21\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m 1/48\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m5:26\u001B[0m 7s/step - accuracy: 0.2500 - loss: 1.4599 - precision_15: 0.2917 - recall_15: 0.2188",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "W0000 00:00:1711219574.999120    2242 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 377ms/step - accuracy: 0.2825 - loss: 1.2019 - precision_15: 0.3048 - recall_15: 0.1176",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "W0000 00:00:1711219594.145853    2242 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m29s\u001B[0m 479ms/step - accuracy: 0.2838 - loss: 1.2007 - precision_15: 0.3059 - recall_15: 0.1179 - val_accuracy: 0.4479 - val_loss: 1.0518 - val_precision_15: 0.0000e+00 - val_recall_15: 0.0000e+00\nEpoch 2/21\n\u001B[1m 1/48\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m7s\u001B[0m 162ms/step - accuracy: 0.3750 - loss: 1.0115 - precision_15: 0.2857 - recall_15: 0.0625",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n  self.gen.throw(typ, value, traceback)\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 14ms/step - accuracy: 0.3750 - loss: 1.0115 - precision_15: 0.2857 - recall_15: 0.0625 - val_accuracy: 0.9130 - val_loss: 0.7078 - val_precision_15: 1.0000 - val_recall_15: 0.6087\nEpoch 3/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 427ms/step - accuracy: 0.4541 - loss: 1.0191 - precision_15: 0.4833 - recall_15: 0.1857 - val_accuracy: 0.5573 - val_loss: 0.9477 - val_precision_15: 0.5573 - val_recall_15: 0.1393\nEpoch 4/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.4688 - loss: 1.0663 - precision_15: 0.3333 - recall_15: 0.1875 - val_accuracy: 0.8696 - val_loss: 0.6411 - val_precision_15: 1.0000 - val_recall_15: 0.6087\nEpoch 5/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 417ms/step - accuracy: 0.4786 - loss: 0.9590 - precision_15: 0.5422 - recall_15: 0.2833 - val_accuracy: 0.6680 - val_loss: 0.8857 - val_precision_15: 0.7182 - val_recall_15: 0.3086\nEpoch 6/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5625 - loss: 0.8954 - precision_15: 0.6500 - recall_15: 0.4062 - val_accuracy: 0.8261 - val_loss: 0.5990 - val_precision_15: 1.0000 - val_recall_15: 0.6522\nEpoch 7/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 423ms/step - accuracy: 0.5751 - loss: 0.9104 - precision_15: 0.6541 - recall_15: 0.3366 - val_accuracy: 0.5690 - val_loss: 0.8752 - val_precision_15: 0.6342 - val_recall_15: 0.3477\nEpoch 8/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5938 - loss: 0.8872 - precision_15: 0.7143 - recall_15: 0.4688 - val_accuracy: 0.9130 - val_loss: 0.5835 - val_precision_15: 1.0000 - val_recall_15: 0.6957\nEpoch 9/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 428ms/step - accuracy: 0.5148 - loss: 0.9046 - precision_15: 0.5938 - recall_15: 0.3377 - val_accuracy: 0.6706 - val_loss: 0.8143 - val_precision_15: 0.7575 - val_recall_15: 0.4596\nEpoch 10/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5000 - loss: 0.9794 - precision_15: 0.4286 - recall_15: 0.1875 - val_accuracy: 0.8696 - val_loss: 0.5825 - val_precision_15: 0.9412 - val_recall_15: 0.6957\nEpoch 11/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 431ms/step - accuracy: 0.5732 - loss: 0.8843 - precision_15: 0.6677 - recall_15: 0.3979 - val_accuracy: 0.6901 - val_loss: 0.7958 - val_precision_15: 0.7933 - val_recall_15: 0.5247\nEpoch 12/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6562 - loss: 0.9223 - precision_15: 0.7000 - recall_15: 0.4375 - val_accuracy: 0.8261 - val_loss: 0.5551 - val_precision_15: 0.9412 - val_recall_15: 0.6957\nEpoch 13/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 422ms/step - accuracy: 0.5820 - loss: 0.8621 - precision_15: 0.6840 - recall_15: 0.4178 - val_accuracy: 0.5534 - val_loss: 0.8618 - val_precision_15: 0.5972 - val_recall_15: 0.3958\nEpoch 14/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5000 - loss: 1.0040 - precision_15: 0.5217 - recall_15: 0.3750 - val_accuracy: 0.9130 - val_loss: 0.4891 - val_precision_15: 0.9474 - val_recall_15: 0.7826\nEpoch 15/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 421ms/step - accuracy: 0.6097 - loss: 0.8018 - precision_15: 0.6980 - recall_15: 0.4633 - val_accuracy: 0.6654 - val_loss: 0.7970 - val_precision_15: 0.7643 - val_recall_15: 0.5234\nEpoch 16/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5938 - loss: 0.8046 - precision_15: 0.6364 - recall_15: 0.4375 - val_accuracy: 0.9130 - val_loss: 0.4949 - val_precision_15: 0.9474 - val_recall_15: 0.7826\nEpoch 17/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 428ms/step - accuracy: 0.6234 - loss: 0.7970 - precision_15: 0.7107 - recall_15: 0.4761 - val_accuracy: 0.6719 - val_loss: 0.7559 - val_precision_15: 0.7496 - val_recall_15: 0.5456\nEpoch 18/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5938 - loss: 0.7454 - precision_15: 0.7273 - recall_15: 0.5000 - val_accuracy: 0.9130 - val_loss: 0.5035 - val_precision_15: 0.9474 - val_recall_15: 0.7826\nEpoch 19/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 428ms/step - accuracy: 0.6617 - loss: 0.7718 - precision_15: 0.7542 - recall_15: 0.5232 - val_accuracy: 0.6667 - val_loss: 0.7358 - val_precision_15: 0.7241 - val_recall_15: 0.5469\nEpoch 20/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6250 - loss: 0.8189 - precision_15: 0.7083 - recall_15: 0.5312 - val_accuracy: 0.8696 - val_loss: 0.5044 - val_precision_15: 0.9474 - val_recall_15: 0.7826\nEpoch 21/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m40s\u001B[0m 421ms/step - accuracy: 0.6454 - loss: 0.7768 - precision_15: 0.7166 - recall_15: 0.5078 - val_accuracy: 0.6979 - val_loss: 0.7113 - val_precision_15: 0.7772 - val_recall_15: 0.5859\n",
     "output_type": "stream"
    },
    {
     "execution_count": 231,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.src.callbacks.history.History at 0x7b5b49209480>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(f\"Test Precision: {test_precision}\")\nprint(f\"Test Recall: {test_recall}\")\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:50:50.549551Z",
     "iopub.execute_input": "2024-03-23T18:50:50.550363Z",
     "iopub.status.idle": "2024-03-23T18:50:54.392245Z",
     "shell.execute_reply.started": "2024-03-23T18:50:50.550313Z",
     "shell.execute_reply": "2024-03-23T18:50:54.391250Z"
    },
    "trusted": true
   },
   "execution_count": 232,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001B[1m24/24\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 153ms/step - accuracy: 0.6267 - loss: 0.7750 - precision_15: 0.6943 - recall_15: 0.5138\nTest Loss: 0.7112874984741211\nTest Accuracy: 0.6979166865348816\nTest Precision: 0.7772020697593689\nTest Recall: 0.5859375\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n\n# Callback for early stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Callback for model checkpointing\nmodel_checkpoint = ModelCheckpoint(\n    'best_model.keras', monitor='val_loss', save_best_only=True, save_weights_only=False)\n\n# Learning rate scheduler\ndef scheduler(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return float(lr * np.exp(-0.1))\n\nlr_schedule = LearningRateScheduler(scheduler)\n\n\nmodel.compile(optimizer=Adam(learning_rate=0.00001), loss='categorical_crossentropy', metrics=['accuracy', Precision(), Recall()])\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n    epochs=21,\n    validation_data=test_generator,\n    validation_steps=test_generator.samples // test_generator.batch_size,\n    class_weight=class_weights_dict,\n    callbacks=[early_stopping, model_checkpoint, lr_schedule]\n)\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:56:28.171318Z",
     "iopub.execute_input": "2024-03-23T18:56:28.172220Z",
     "iopub.status.idle": "2024-03-23T18:58:11.309909Z",
     "shell.execute_reply.started": "2024-03-23T18:56:28.172187Z",
     "shell.execute_reply": "2024-03-23T18:58:11.308729Z"
    },
    "trusted": true
   },
   "execution_count": 237,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1/21\n\u001B[1m 1/48\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m5:23\u001B[0m 7s/step - accuracy: 0.6562 - loss: 0.6754 - precision_19: 0.7083 - recall_19: 0.5312",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "W0000 00:00:1711220195.296441    2243 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 372ms/step - accuracy: 0.6704 - loss: 0.7348 - precision_19: 0.7435 - recall_19: 0.5376",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "W0000 00:00:1711220214.164946    2244 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 484ms/step - accuracy: 0.6704 - loss: 0.7350 - precision_19: 0.7436 - recall_19: 0.5378 - val_accuracy: 0.6706 - val_loss: 0.7321 - val_precision_19: 0.7543 - val_recall_19: 0.5755 - learning_rate: 1.0000e-05\nEpoch 2/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 22ms/step - accuracy: 0.8125 - loss: 0.6381 - precision_19: 0.9545 - recall_19: 0.6562 - val_accuracy: 0.9130 - val_loss: 0.4414 - val_precision_19: 0.9500 - val_recall_19: 0.8261 - learning_rate: 1.0000e-05\nEpoch 3/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 426ms/step - accuracy: 0.6549 - loss: 0.7330 - precision_19: 0.7282 - recall_19: 0.5199 - val_accuracy: 0.6641 - val_loss: 0.7298 - val_precision_19: 0.7319 - val_recall_19: 0.5651 - learning_rate: 1.0000e-05\nEpoch 4/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.5312 - loss: 0.7157 - precision_19: 0.6154 - recall_19: 0.5000 - val_accuracy: 0.9130 - val_loss: 0.4503 - val_precision_19: 0.9500 - val_recall_19: 0.8261 - learning_rate: 1.0000e-05\nEpoch 5/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 423ms/step - accuracy: 0.6429 - loss: 0.7485 - precision_19: 0.7231 - recall_19: 0.5175 - val_accuracy: 0.6862 - val_loss: 0.7183 - val_precision_19: 0.7675 - val_recall_19: 0.5846 - learning_rate: 1.0000e-05\nEpoch 6/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - accuracy: 0.6562 - loss: 0.7762 - precision_19: 0.7727 - recall_19: 0.5312 - val_accuracy: 0.9130 - val_loss: 0.4515 - val_precision_19: 0.9500 - val_recall_19: 0.8261 - learning_rate: 1.0000e-05\nEpoch 7/21\n\u001B[1m48/48\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m23s\u001B[0m 429ms/step - accuracy: 0.6989 - loss: 0.7188 - precision_19: 0.7654 - recall_19: 0.5518 - val_accuracy: 0.7005 - val_loss: 0.7052 - val_precision_19: 0.7742 - val_recall_19: 0.5938 - learning_rate: 1.0000e-05\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_generator, steps=test_generator.samples // test_generator.batch_size)\n\nprint(f\"Test Loss: {test_loss}\")\nprint(f\"Test Accuracy: {test_accuracy}\")\nprint(f\"Test Precision: {test_precision}\")\nprint(f\"Test Recall: {test_recall}\")\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-03-23T18:58:22.175713Z",
     "iopub.execute_input": "2024-03-23T18:58:22.176116Z",
     "iopub.status.idle": "2024-03-23T18:58:25.923891Z",
     "shell.execute_reply.started": "2024-03-23T18:58:22.176081Z",
     "shell.execute_reply": "2024-03-23T18:58:25.922878Z"
    },
    "trusted": true
   },
   "execution_count": 238,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001B[1m24/24\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 146ms/step - accuracy: 0.5989 - loss: 0.8078 - precision_19: 0.6536 - recall_19: 0.4934\nTest Loss: 0.7326685786247253\nTest Accuracy: 0.66796875\nTest Precision: 0.7465986609458923\nTest Recall: 0.5716145634651184\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
